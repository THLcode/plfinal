// import React, { useState, useRef } from "react";
// import Webcam from "react-webcam";
// import styled, { keyframes } from "styled-components";
// import AWS from "aws-sdk";

// // Keyframe for spinner animation
// const spin = keyframes`
//   0% { transform: rotate(0deg); }
//   100% { transform: rotate(360deg); }
// `;

// // Styled Components
// const PageWrapper = styled.div`
//   background: linear-gradient(135deg, #e0f2fe 0%, #fef9c3 100%);
//   min-height: 100vh;
//   padding: 2rem;
//   display: flex;
//   flex-direction: column;
//   align-items: center;
// `;

// const Header = styled.header`
//   background: #fff;
//   width: 100%;
//   max-width: 1200px;
//   padding: 1.5rem 2rem;
//   border-radius: 0.75rem;
//   box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
//   margin-bottom: 2.5rem;
//   text-align: center;
// `;

// const Title = styled.h1`
//   margin: 0;
//   font-size: 2.75rem;
//   color: #3b82f6;
// `;

// const Card = styled.div`
//   background: #ffffff;
//   width: 100%;
//   max-width: 1200px;
//   border-radius: 1rem;
//   box-shadow: 0 12px 36px rgba(0, 0, 0, 0.1);
//   padding: 2.5rem;
// `;

// const FlexGrid = styled.div`
//   display: flex;
//   gap: 2rem;
// `;

// const Section = styled.div`
//   flex: ${(props) => props.flex || 1};
//   display: flex;
//   flex-direction: column;
//   align-items: center;
//   min-width: 320px;
// `;

// const WebcamContainer = styled.div`
//   width: 100%;
//   height: 500px;
//   border-radius: 1rem;
//   overflow: hidden;
//   border: 2px solid #d1d5db;
//   margin-bottom: 1.5rem;
// `;

// const Button = styled.button`
//   display: inline-flex;
//   align-items: center;
//   gap: 0.5rem;
//   background: ${({ bg }) => bg || "#3b82f6"};
//   color: #fff;
//   padding: 1rem 1.75rem;
//   font-size: 1.125rem;
//   font-weight: 600;
//   border: none;
//   border-radius: 0.75rem;
//   cursor: ${({ disabled }) => (disabled ? "not-allowed" : "pointer")};
//   box-shadow: 0 2px 6px rgba(0, 0, 0, 0.1);
//   transition: background 0.3s, transform 0.1s;
//   &:hover {
//     background: ${({ hover }) => hover || "#2563eb"};
//     transform: ${({ disabled }) => (disabled ? "none" : "translateY(-2px)")};
//   }
// `;

// const Subtitle = styled.p`
//   margin-top: 1.25rem;
//   font-size: 1.125rem;
//   color: #374151;
//   text-align: center;
// `;

// const CodeSection = styled.div`
//   margin-top: 3rem;
//   text-align: center;
// `;

// const CodeHeader = styled.h2`
//   font-size: 1.75rem;
//   color: #1f2937;
//   margin-bottom: 1rem;
// `;

// const CodeBlock = styled.pre`
//   background: #111827;
//   color: #d1d5db;
//   padding: 1.75rem;
//   border-radius: 0.75rem;
//   font-family: "Source Code Pro", monospace;
//   font-size: 0.875rem;
//   max-height: 350px;
//   overflow: auto;
// `;

// const Spinner = styled.div`
//   width: 1.25rem;
//   height: 1.25rem;
//   border: 3px solid rgba(255, 255, 255, 0.3);
//   border-top: 3px solid #fff;
//   border-radius: 50%;
//   animation: ${spin} 1s linear infinite;
// `;

// const EmotionCodeGenerator = () => {
//   const webcamRef = useRef(null);
//   const mediaRecorderRef = useRef(null);
//   const audioChunksRef = useRef([]);

//   const [emotion, setEmotion] = useState("");
//   const [transcript, setTranscript] = useState("");
//   const [code, setCode] = useState("");
//   const [isAnalyzing, setIsAnalyzing] = useState(false);

//   const [isListening, setIsListening] = useState(false);
//   const [mediaStream, setMediaStream] = useState(null);

//   const [isGenerating, setIsGenerating] = useState(false);

//   const rekognition = new AWS.Rekognition({
//     accessKeyId: process.env.REACT_APP_AWS_ACCESS_KEY_ID,
//     secretAccessKey: process.env.REACT_APP_AWS_SECRET_ACCESS_KEY,
//     region: process.env.REACT_APP_AWS_REGION,
//   });

//   // Capture webcam image and detect emotion via Azure Face API
//   const detectEmotion = async () => {
//     if (!webcamRef.current) return;
//     setIsAnalyzing(true);
//     try {
//       const screenshot = webcamRef.current.getScreenshot();
//       const blob = await (await fetch(screenshot)).blob();
//       const buffer = await blob.arrayBuffer();

//       rekognition.detectFaces(
//         {
//           Image: { Bytes: new Uint8Array(buffer) },
//           Attributes: ["ALL"],
//         },
//         (err, data) => {
//           if (err) {
//             console.error("AWS Rekognition error:", err);
//             setEmotion("ê°ì • ì¸ì‹ ì‹¤íŒ¨");
//           } else if (
//             data.FaceDetails &&
//             data.FaceDetails[0] &&
//             data.FaceDetails[0].Emotions
//           ) {
//             const emotions = data.FaceDetails[0].Emotions;
//             const top = emotions.reduce((a, b) =>
//               a.Confidence > b.Confidence ? a : b
//             );
//             const emojiMap = {
//               HAPPY: "ğŸ˜„",
//               SAD: "ğŸ˜¢",
//               CALM: "ğŸ˜",
//               ANGRY: "ğŸ˜ ",
//               FEAR: "ğŸ˜¨",
//               DISGUSTED: "ğŸ¤¢",
//               SURPRISED: "ğŸ˜²",
//               CONFUSED: "ğŸ¤”",
//             };
//             setEmotion(`${emojiMap[top.Type] || ""} ${top.Type}`);
//           } else {
//             setEmotion("ğŸ˜ neutral");
//           }
//         }
//       );
//     } catch (e) {
//       console.error("Emotion detection failed", e);
//     }
//     setIsAnalyzing(false);
//   };

//   // Record audio and transcribe via OpenAI Whisper
//   const transcribeAudio = async (file) => {
//     const formData = new FormData();
//     formData.append("file", file);
//     formData.append("model", "whisper-1");
//     const res = await fetch("https://api.openai.com/v1/audio/transcriptions", {
//       method: "POST",
//       headers: {
//         Authorization: `Bearer ${process.env.REACT_APP_OPENAI_API_KEY}`,
//       },
//       body: formData,
//     });
//     const json = await res.json();
//     return json.text;
//   };

//   const startListening = async () => {
//     if (isListening) {
//       // ë…¹ìŒ ì¤‘ì´ë©´ ì¤‘ì§€
//       if (mediaRecorderRef.current && mediaRecorderRef.current.state !== "inactive") {
//         mediaRecorderRef.current.stop();
//       }
//       setIsListening(false);
//     } else {
//       // ë…¹ìŒ ì‹œì‘
//       try {
//         const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
//         setMediaStream(stream);
//         const recorder = new MediaRecorder(stream);
//         mediaRecorderRef.current = recorder;
//         audioChunksRef.current = [];
//         recorder.ondataavailable = (e) => {
//           if (e.data.size > 0) audioChunksRef.current.push(e.data);
//         };
//         recorder.onstop = async () => {
//           // ë…¹ìŒ ëë‚˜ë©´ stream í•´ì œ
//           stream.getTracks().forEach((track) => track.stop());
//           const audioBlob = new Blob(audioChunksRef.current, {
//             type: "audio/webm",
//           });
//           const audioFile = new File([audioBlob], "recording.webm", {
//             type: "audio/webm",
//           });
//           const text = await transcribeAudio(audioFile);
//           setTranscript(text);
//           setIsListening(false);
//           setMediaStream(null);
//         };
//         recorder.start();
//         setIsListening(true);
//       } catch (e) {
//         console.error("Recording failed", e);
//         setIsListening(false);
//       }
//     }
//   };

//   // Generate code via OpenAI Chat
//   const generateCode = async () => {
//     if (!emotion || !transcript) return;
//     setIsGenerating(true);
//     try {
//       const messages = [
//         { role: "system", content: "You are a helpful code generator." },
//         {
//           role: "user",
//           content: `Emotion: ${emotion}\nRequest: ${transcript}\nGenerate Python code:`,
//         },
//       ];
//       const res = await fetch("https://api.openai.com/v1/chat/completions", {
//         method: "POST",
//         headers: {
//           "Content-Type": "application/json",
//           Authorization: `Bearer ${process.env.REACT_APP_OPENAI_API_KEY}`,
//         },
//         body: JSON.stringify({ model: "gpt-4", messages, max_tokens: 500 }),
//       });
//       const json = await res.json();
//       setCode(json.choices[0].message.content);
//     } catch (e) {
//       console.error("Code generation failed", e);
//     }
//     setIsGenerating(false);
//   };

//   return (
//     <PageWrapper>
//       <Header>
//         <Title>ê°ì • ê¸°ë°˜ ì½”ë“œ ìƒì„±ê¸°</Title>
//       </Header>
//       <Card>
//         <FlexGrid>
//           <Section flex={2}>
//             <WebcamContainer>
//               <Webcam
//                 ref={webcamRef}
//                 screenshotFormat="image/jpeg"
//                 style={{ width: "100%", height: "100%", objectFit: "cover" }}
//               />
//             </WebcamContainer>
//             <Button
//               onClick={detectEmotion}
//               disabled={isAnalyzing}
//               bg="#3b82f6"
//               hover="#2563eb"
//             >
//               {isAnalyzing ? <Spinner /> : "ê°ì • ë¶„ì„"}
//             </Button>
//             <Subtitle>
//               í˜„ì¬ ê°ì •: <strong>{emotion || "ì—†ìŒ"}</strong>
//             </Subtitle>
//           </Section>
//           <Section flex={1}>
//             <Button
//               onClick={startListening}
//               disabled={isGenerating}
//               bg="#10b981"
//               hover="#059669"
//             >
//               {isListening ? "ë…¹ìŒ ì¤‘ì§€" : "ìŒì„± ìš”ì²­"}
//               {isListening && <Spinner />}
//             </Button>
//             <Subtitle>
//               ìš”ì²­ ë‚´ìš©: <strong>{transcript || "ì—†ìŒ"}</strong>
//             </Subtitle>
//             <Button
//               onClick={generateCode}
//               disabled={!emotion || !transcript || isGenerating}
//               bg="#f59e0b"
//               hover="#d97706"
//               style={{ marginTop: "1.5rem" }}
//             >
//               {isGenerating ? <Spinner /> : "ì½”ë“œ ìƒì„±"}
//             </Button>
//           </Section>
//         </FlexGrid>
//         <CodeSection>
//           <CodeHeader>ìƒì„±ëœ ì½”ë“œ:</CodeHeader>
//           <CodeBlock>{code || "í˜„ì¬ ìƒì„±ëœ ì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤."}</CodeBlock>
//         </CodeSection>
//       </Card>
//     </PageWrapper>
//   );
// };

// export default EmotionCodeGenerator;
import React, { useState, useRef } from "react";
import Webcam from "react-webcam";
import styled, { keyframes } from "styled-components";
import AWS from "aws-sdk";
import AceEditor from "react-ace";
import "ace-builds/src-noconflict/mode-python";
import "ace-builds/src-noconflict/theme-monokai";

// Keyframe for spinner animation
const spin = keyframes`
  0% { transform: rotate(0deg); }
  100% { transform: rotate(360deg); }
`;

// Styled Components
const PageWrapper = styled.div`
  background: linear-gradient(135deg, #e0f2fe 0%, #fef9c3 100%);
  min-height: 100vh;
  padding: 2rem;
  display: flex;
  flex-direction: column;
  align-items: center;
`;

const Header = styled.header`
  background: #fff;
  width: 100%;
  max-width: 1200px;
  padding: 1.5rem 2rem;
  border-radius: 0.75rem;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
  margin-bottom: 2.5rem;
  text-align: center;
`;

const Title = styled.h1`
  margin: 0;
  font-size: 2.75rem;
  color: #3b82f6;
`;

const Card = styled.div`
  background: #ffffff;
  width: 100%;
  max-width: 1200px;
  border-radius: 1rem;
  box-shadow: 0 12px 36px rgba(0, 0, 0, 0.1);
  padding: 2.5rem;
`;

const FlexGrid = styled.div`
  display: flex;
  gap: 2rem;
`;

const Section = styled.div`
  flex: ${(props) => props.flex || 1};
  display: flex;
  flex-direction: column;
  align-items: center;
  min-width: 320px;
`;

const WebcamContainer = styled.div`
  width: 100%;
  height: 500px;
  border-radius: 1rem;
  overflow: hidden;
  border: 2px solid #d1d5db;
  margin-bottom: 1.5rem;
`;

const Button = styled.button`
  display: inline-flex;
  align-items: center;
  gap: 0.5rem;
  background: ${({ bg }) => bg || "#3b82f6"};
  color: #fff;
  padding: 1rem 1.75rem;
  font-size: 1.125rem;
  font-weight: 600;
  border: none;
  border-radius: 0.75rem;
  cursor: ${({ disabled }) => (disabled ? "not-allowed" : "pointer")};
  box-shadow: 0 2px 6px rgba(0, 0, 0, 0.1);
  transition: background 0.3s, transform 0.1s;
  &:hover {
    background: ${({ hover }) => hover || "#2563eb"};
    transform: ${({ disabled }) => (disabled ? "none" : "translateY(-2px)")};
  }
`;

const Subtitle = styled.p`
  margin-top: 1.25rem;
  font-size: 1.125rem;
  color: #374151;
  text-align: center;
`;

const CodeSection = styled.div`
  margin-top: 3rem;
  text-align: center;
`;

const CodeHeader = styled.h2`
  font-size: 1.75rem;
  color: #1f2937;
  margin-bottom: 1rem;
`;

const Spinner = styled.div`
  width: 1.25rem;
  height: 1.25rem;
  border: 3px solid rgba(255, 255, 255, 0.3);
  border-top: 3px solid #fff;
  border-radius: 50%;
  animation: ${spin} 1s linear infinite;
`;

const EmotionCodeGenerator = () => {
  const webcamRef = useRef(null);
  const mediaRecorderRef = useRef(null);
  const audioChunksRef = useRef([]);

  const [emotion, setEmotion] = useState("");
  const [transcript, setTranscript] = useState("");
  const [code, setCode] = useState("");
  const [isAnalyzing, setIsAnalyzing] = useState(false);
  const [isListening, setIsListening] = useState(false);
  const [mediaStream, setMediaStream] = useState(null);
  const [isGenerating, setIsGenerating] = useState(false);

  const rekognition = new AWS.Rekognition({
    accessKeyId: process.env.REACT_APP_AWS_ACCESS_KEY_ID,
    secretAccessKey: process.env.REACT_APP_AWS_SECRET_ACCESS_KEY,
    region: process.env.REACT_APP_AWS_REGION,
  });

  // Capture webcam image and detect emotion via AWS Rekognition
  const detectEmotion = async () => {
    if (!webcamRef.current) return;
    setIsAnalyzing(true);
    try {
      const screenshot = webcamRef.current.getScreenshot();
      const blob = await (await fetch(screenshot)).blob();
      const buffer = await blob.arrayBuffer();

      rekognition.detectFaces(
        {
          Image: { Bytes: new Uint8Array(buffer) },
          Attributes: ["ALL"],
        },
        (err, data) => {
          if (err) {
            console.error("AWS Rekognition error:", err);
            setEmotion("ê°ì • ì¸ì‹ ì‹¤íŒ¨");
          } else if (
            data.FaceDetails &&
            data.FaceDetails[0] &&
            data.FaceDetails[0].Emotions
          ) {
            const emotions = data.FaceDetails[0].Emotions;
            const top = emotions.reduce((a, b) =>
              a.Confidence > b.Confidence ? a : b
            );
            const emojiMap = {
              HAPPY: "ğŸ˜„",
              SAD: "ğŸ˜¢",
              CALM: "ğŸ˜",
              ANGRY: "ğŸ˜ ",
              FEAR: "ğŸ˜¨",
              DISGUSTED: "ğŸ¤¢",
              SURPRISED: "ğŸ˜²",
              CONFUSED: "ğŸ¤”",
            };
            setEmotion(`${emojiMap[top.Type] || ""} ${top.Type}`);
          } else {
            setEmotion("ğŸ˜ neutral");
          }
        }
      );
    } catch (e) {
      console.error("Emotion detection failed", e);
    }
    setIsAnalyzing(false);
  };

  // Record audio and transcribe via OpenAI Whisper
  const transcribeAudio = async (file) => {
    const formData = new FormData();
    formData.append("file", file);
    formData.append("model", "whisper-1");
    const res = await fetch("https://api.openai.com/v1/audio/transcriptions", {
      method: "POST",
      headers: {
        Authorization: `Bearer ${process.env.REACT_APP_OPENAI_API_KEY}`,
      },
      body: formData,
    });
    const json = await res.json();
    return json.text;
  };

  const startListening = async () => {
    if (isListening) {
      if (
        mediaRecorderRef.current &&
        mediaRecorderRef.current.state !== "inactive"
      ) {
        mediaRecorderRef.current.stop();
      }
      setIsListening(false);
    } else {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({
          audio: true,
        });
        setMediaStream(stream);
        const recorder = new MediaRecorder(stream);
        mediaRecorderRef.current = recorder;
        audioChunksRef.current = [];
        recorder.ondataavailable = (e) => {
          if (e.data.size > 0) audioChunksRef.current.push(e.data);
        };
        recorder.onstop = async () => {
          stream.getTracks().forEach((track) => track.stop());
          const audioBlob = new Blob(audioChunksRef.current, {
            type: "audio/webm",
          });
          const audioFile = new File([audioBlob], "recording.webm", {
            type: "audio/webm",
          });
          const text = await transcribeAudio(audioFile);
          setTranscript(text);
          setIsListening(false);
          setMediaStream(null);
        };
        recorder.start();
        setIsListening(true);
      } catch (e) {
        console.error("Recording failed", e);
        setIsListening(false);
      }
    }
  };

  // Generate code via OpenAI Chat with emotion-based prompt
  const generateCode = async () => {
    if (!emotion || !transcript) return;
    setIsGenerating(true);

    // ê°ì •ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì¡°ì •
    let promptTemplate = "";
    switch (true) {
      case emotion.includes("HAPPY"):
        promptTemplate = `You are a cheerful code generator. Add happy emojis to comments and keep the code simple and fun. Here's the request: "${transcript}"`;
        break;
      case emotion.includes("SAD"):
        promptTemplate = `You are a supportive code generator. Add comforting comments and provide detailed explanations in the code. Here's the request: "${transcript}"`;
        break;
      case emotion.includes("ANGRY"):
        promptTemplate = `You are a calm code generator. Add soothing comments to help the user relax and provide concise code. Here's the request: "${transcript}"`;
        break;
      case emotion.includes("CONFUSED"):
        promptTemplate = `You are a patient code generator. Add step-by-step explanations in the comments and break down the code into smaller parts. Here's the request: "${transcript}"`;
        break;
      case emotion.includes("DISGUSTED"):
        promptTemplate = `You are a humorous code generator. Add funny comments to lighten the mood and keep the code simple. Here's the request: "${transcript}"`;
        break;
      case emotion.includes("SURPRISED"):
        promptTemplate = `You are an enthusiastic code generator. Add exciting comments and provide concise code. Here's the request: "${transcript}"`;
        break;
      case emotion.includes("CALM"):
        promptTemplate = `You are a serene code generator. Add peaceful comments and provide clear, well-structured code. Here's the request: "${transcript}"`;
        break;
      case emotion.includes("FEAR"):
        promptTemplate = `You are a reassuring code generator. Add encouraging comments and provide detailed, easy-to-understand code. Here's the request: "${transcript}"`;
        break;
      default:
        promptTemplate = `You are a helpful code generator. Provide clear and concise code with appropriate comments. Here's the request: "${transcript}"`;
        break;
    }

    try {
      const messages = [
        { role: "system", content: promptTemplate },
        {
          role: "user",
          content: `Generate Python code for the following request: ${transcript}`,
        },
      ];
      const res = await fetch("https://api.openai.com/v1/chat/completions", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          Authorization: `Bearer ${process.env.REACT_APP_OPENAI_API_KEY}`,
        },
        body: JSON.stringify({ model: "gpt-4", messages, max_tokens: 500 }),
      });
      const json = await res.json();
      setCode(json.choices[0].message.content);
    } catch (e) {
      console.error("Code generation failed", e);
    }
    setIsGenerating(false);
  };

  return (
    <PageWrapper>
      <Header>
        <Title>ê°ì • ê¸°ë°˜ ì½”ë“œ ìƒì„±ê¸°</Title>
      </Header>
      <Card>
        <FlexGrid>
          <Section flex={2}>
            <WebcamContainer>
              <Webcam
                ref={webcamRef}
                screenshotFormat="image/jpeg"
                style={{ width: "100%", height: "100%", objectFit: "cover" }}
              />
            </WebcamContainer>
            <Button
              onClick={detectEmotion}
              disabled={isAnalyzing}
              bg="#3b82f6"
              hover="#2563eb"
            >
              {isAnalyzing ? <Spinner /> : "ê°ì • ë¶„ì„"}
            </Button>
            <Subtitle>
              í˜„ì¬ ê°ì •: <strong>{emotion || "ì—†ìŒ"}</strong>
            </Subtitle>
          </Section>
          <Section flex={1}>
            <Button
              onClick={startListening}
              disabled={isGenerating}
              bg="#10b981"
              hover="#059669"
            >
              {isListening ? "ë…¹ìŒ ì¤‘ì§€" : "ìŒì„± ìš”ì²­"}
              {isListening && <Spinner />}
            </Button>
            <Subtitle>
              ìš”ì²­ ë‚´ìš©: <strong>{transcript || "ì—†ìŒ"}</strong>
            </Subtitle>
            <Button
              onClick={generateCode}
              disabled={!emotion || !transcript || isGenerating}
              bg="#f59e0b"
              hover="#d97706"
              style={{ marginTop: "1.5rem" }}
            >
              {isGenerating ? <Spinner /> : "ì½”ë“œ ìƒì„±"}
            </Button>
          </Section>
        </FlexGrid>
        <CodeSection>
          <CodeHeader>ìƒì„±ëœ ì½”ë“œ:</CodeHeader>
          <AceEditor
            mode="python"
            theme="monokai"
            value={code || "í˜„ì¬ ìƒì„±ëœ ì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤."}
            width="100%"
            height="350px"
            readOnly
            setOptions={{
              showLineNumbers: true,
              tabSize: 4,
            }}
          />
        </CodeSection>
      </Card>
    </PageWrapper>
  );
};

export default EmotionCodeGenerator;
